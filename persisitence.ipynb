{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40ab0659",
   "metadata": {},
   "source": [
    "# `human in the loop`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3486f62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyDyiYKF2iqjoDnJBkmidZSnd7Ic6dzU_Ls\"\n",
    "\n",
    "llm = init_chat_model(\"google_genai:gemini-2.0-flash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c92174",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "\n",
    "# from langchain_tavily import TavilySearch\n",
    "from langchain_core.tools import tool\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "from langgraph.types import Command, interrupt\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    human_response : str\n",
    "\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "@tool\n",
    "def human_assistance(query: str) -> str:\n",
    "    \"\"\"Request assistance from a human.\"\"\"\n",
    "\n",
    "    human_response = interrupt({\"query\": query})\n",
    "\n",
    "    return human_response[\"data\"]\n",
    "\n",
    "# tool = TavilySearch(max_results=2)\n",
    "tools = [human_assistance]\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "def chatbot(state: State):\n",
    "    message = llm_with_tools.invoke(state[\"messages\"])\n",
    "    # Because we will be interrupting during tool execution,\n",
    "    # we disable parallel tool calling to avoid repeating any\n",
    "    # tool invocations when we resume.\n",
    "    assert len(message.tool_calls) <= 1\n",
    "    return {\"messages\": [message]}\n",
    "\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "\n",
    "tool_node = ToolNode(tools=tools)\n",
    "graph_builder.add_node(\"tools\", tool_node)\n",
    "\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"chatbot\",\n",
    "    tools_condition,\n",
    ")\n",
    "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
    "graph_builder.add_edge(START, \"chatbot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0561111a",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = InMemorySaver()\n",
    "\n",
    "graph = graph_builder.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752419e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027073fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = \"I need some expert guidance for building an AI agent. Could you request assistance for me? don;t ask more questions only call the assistence for me\"\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "events = graph.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": user_input}]},\n",
    "    config,\n",
    ")\n",
    "for event in events:\n",
    "    for value in event.values():\n",
    "        if \"messages\" in value and value['messages']:\n",
    "            last_msg = value[\"messages\"][-1]\n",
    "            print(last_msg)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185b7bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshot = graph.get_state(config)\n",
    "snapshot.next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50030110",
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a121be93",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(graph.get_state_history(config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34032bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_response = (\n",
    "    \"We, the experts are here to help! We'd recommend you check out LangGraph to build your agent.\"\n",
    "    \" It's much more reliable and extensible than simple autonomous agents.\"\n",
    ")\n",
    "\n",
    "human_command = Command(resume={\"data\": human_response})\n",
    "\n",
    "events = graph.stream(human_command, config, stream_mode=\"values\")\n",
    "for event in events:\n",
    "    if \"messages\" in event:\n",
    "        event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea20c59e",
   "metadata": {},
   "source": [
    "# `with more tools`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88f9e7a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='' additional_kwargs={'function_call': {'name': 'GMAIL_SEND_EMAIL', 'arguments': '{\"recipient_email\": \"tybhsn001@gmail.com\", \"body\": \"hey call me\"}'}} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []} id='run--2dc22b64-8e12-4e85-8014-ac80b5c05363-0' tool_calls=[{'name': 'human_assistance', 'args': {'query': 'Expert guidance for building an AI agent.'}, 'id': '9804f4a2-f20b-4f56-92be-21b74db0f623', 'type': 'tool_call'}, {'name': 'GMAIL_SEND_EMAIL', 'args': {'recipient_email': 'tybhsn001@gmail.com', 'body': 'hey call me'}, 'id': 'c645e0d2-d38b-4781-a627-98697a6dd472', 'type': 'tool_call'}] usage_metadata={'input_tokens': 742, 'output_tokens': 57, 'total_tokens': 881, 'input_token_details': {'cache_read': 656}}\n",
      "Next steps: ('tools',)\n",
      "Current human_response in state: None\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  human_assistance (9804f4a2-f20b-4f56-92be-21b74db0f623)\n",
      " Call ID: 9804f4a2-f20b-4f56-92be-21b74db0f623\n",
      "  Args:\n",
      "    query: Expert guidance for building an AI agent.\n",
      "  GMAIL_SEND_EMAIL (c645e0d2-d38b-4781-a627-98697a6dd472)\n",
      " Call ID: c645e0d2-d38b-4781-a627-98697a6dd472\n",
      "  Args:\n",
      "    recipient_email: tybhsn001@gmail.com\n",
      "    body: hey call me\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "\n",
      "use langgraph\n",
      "Stored human response: use langgraph\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I've requested expert guidance for you, and the recommendation is to use LangGraph. I've also sent the email with the content 'hey call me' to tybhsn001@gmail.com.\n",
      "Stored human response: use langgraph\n",
      "Stored human response: use langgraph\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain.chat_models import init_chat_model\n",
    "from typing import Annotated\n",
    "from langchain_core.tools import tool\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langgraph.types import Command, interrupt\n",
    "\n",
    "# Set up your API key\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyDyiYKF2iqjoDnJBkmidZSnd7Ic6dzU_Ls\"\n",
    "llm = init_chat_model(\"google_genai:gemini-2.5-flash\")\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    human_response: str\n",
    "\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "# Set up Composio tools\n",
    "from composio import Composio\n",
    "from composio_langchain import LangchainProvider\n",
    "composio = Composio(provider=LangchainProvider())\n",
    "\n",
    "composio_tools = composio.tools.get(\n",
    "    user_id=\"0000-0000-0000\",  # replace with your composio user_id\n",
    "    tools=[\"TEXT_TO_PDF_CONVERT_TEXT_TO_PDF\", \"GMAIL_SEND_EMAIL\", \"GOOGLEDRIVE_UPLOAD_FILE\"]\n",
    ")\n",
    "\n",
    "@tool\n",
    "def human_assistance(query: str) -> str:\n",
    "    \"\"\"Request assistance from a human.\"\"\"\n",
    "    human_response = interrupt({\"query\": query})\n",
    "    return human_response[\"data\"]\n",
    "\n",
    "# Combine all tools\n",
    "all_tools = [human_assistance] + composio_tools\n",
    "\n",
    "# Custom tool node that handles both human assistance and composio tools\n",
    "def custom_tool_node(state: State):\n",
    "    # Get the last message which should be an AI message with tool calls\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    \n",
    "    if hasattr(last_message, 'tool_calls') and last_message.tool_calls:\n",
    "        tool_call = last_message.tool_calls[0]\n",
    "        \n",
    "        if tool_call[\"name\"] == \"human_assistance\":\n",
    "            # Handle human assistance tool specially\n",
    "            query = tool_call[\"args\"][\"query\"]\n",
    "            human_response_data = interrupt({\"query\": query})\n",
    "            response_text = human_response_data[\"data\"]\n",
    "            \n",
    "            # Create tool message\n",
    "            from langchain_core.messages import ToolMessage\n",
    "            tool_message = ToolMessage(\n",
    "                content=response_text,\n",
    "                tool_call_id=tool_call[\"id\"]\n",
    "            )\n",
    "            \n",
    "            # Return updated state with both the tool message and human response stored\n",
    "            return {\n",
    "                \"messages\": [tool_message],\n",
    "                \"human_response\": response_text\n",
    "            }\n",
    "        else:\n",
    "            # Handle Composio tools using regular ToolNode\n",
    "            composio_tool_node = ToolNode(tools=composio_tools)\n",
    "            result = composio_tool_node.invoke(state)\n",
    "            return result\n",
    "    \n",
    "    # Fallback to regular tool execution if needed\n",
    "    tool_node = ToolNode(tools=all_tools)\n",
    "    result = tool_node.invoke(state)\n",
    "    return result\n",
    "\n",
    "def chatbot(state: State):\n",
    "    # Bind all tools to the LLM\n",
    "    message = llm.bind_tools(all_tools).invoke(state[\"messages\"])\n",
    "    # Because we will be interrupting during tool execution,\n",
    "    # we disable parallel tool calling to avoid repeating any\n",
    "    # tool invocations when we resume.\n",
    "    # assert len(message.tool_calls) <= 1\n",
    "    return {\"messages\": [message]}\n",
    "\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "graph_builder.add_node(\"tools\", custom_tool_node)\n",
    "\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"chatbot\",\n",
    "    tools_condition,\n",
    ")\n",
    "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
    "graph_builder.add_edge(START, \"chatbot\")\n",
    "\n",
    "memory = InMemorySaver()\n",
    "graph = graph_builder.compile(checkpointer=memory)\n",
    "\n",
    "# Test the modified graph\n",
    "if __name__ == \"__main__\":\n",
    "    # Example 1: Using human assistance\n",
    "    user_input = \"I need some expert guidance for building an AI agent. Could you request assistance for me and also send this content 'hey call me' to the user 'tybhsn001@gmail.com'?\"\n",
    "    \n",
    "    # Example 2: Using Composio tools (uncomment to test)\n",
    "    # user_input = \"Please send an email to john@example.com with subject 'Test' and message 'Hello from AI agent'\"\n",
    "    \n",
    "    # Example 3: Converting text to PDF (uncomment to test)\n",
    "    # user_input = \"Convert this text to PDF: 'This is a sample document for testing PDF conversion.'\"\n",
    "    \n",
    "    config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "    # Initial request\n",
    "    events = graph.stream(\n",
    "        {\"messages\": [{\"role\": \"user\", \"content\": user_input}]},\n",
    "        config,\n",
    "    )\n",
    "    for event in events:\n",
    "        for value in event.values():\n",
    "            if \"messages\" in value and value['messages']:\n",
    "                last_msg = value[\"messages\"][-1]\n",
    "                print(last_msg)\n",
    "\n",
    "    # Get current state to see if there's an interruption\n",
    "    snapshot = graph.get_state(config)\n",
    "    print(f\"Next steps: {snapshot.next}\")\n",
    "    print(f\"Current human_response in state: {snapshot.values.get('human_response', 'None')}\")\n",
    "\n",
    "    # If there's an interruption (human assistance needed), handle it\n",
    "    if snapshot.next == ('tools',):\n",
    "        # Check if the last message has a human_assistance tool call\n",
    "        last_message = snapshot.values[\"messages\"][-1]\n",
    "        if (hasattr(last_message, 'tool_calls') and \n",
    "            last_message.tool_calls and \n",
    "            last_message.tool_calls[0][\"name\"] == \"human_assistance\"):\n",
    "            \n",
    "            # Provide human response\n",
    "            human_response = input('assistant_response: ')\n",
    "\n",
    "            human_command = Command(resume={\"data\": human_response})\n",
    "\n",
    "            # Resume execution\n",
    "            events = graph.stream(human_command, config, stream_mode=\"values\")\n",
    "            for event in events:\n",
    "                if \"messages\" in event:\n",
    "                    event[\"messages\"][-1].pretty_print()\n",
    "                # Print the human response stored in state\n",
    "                if \"human_response\" in event:\n",
    "                    print(f\"Stored human response: {event['human_response']}\")\n",
    "\n",
    "            # After providing human response\n",
    "            final_snapshot = graph.get_state(config)\n",
    "            print(f\"Stored human response: {final_snapshot.values.get('human_response')}\")\n",
    "    else:\n",
    "        print(\"No human intervention needed - task completed with available tools.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1df2a5",
   "metadata": {},
   "source": [
    "# `time travel`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "501b2125",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from typing import Annotated\n",
    "\n",
    "from langchain_tavily import TavilySearch\n",
    "from langchain_core.messages import BaseMessage\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "tool = TavilySearch(max_results=2)\n",
    "tools = [tool]\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "def chatbot(state: State):\n",
    "    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n",
    "\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "\n",
    "tool_node = ToolNode(tools=[tool])\n",
    "graph_builder.add_node(\"tools\", tool_node)\n",
    "\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"chatbot\",\n",
    "    tools_condition,\n",
    ")\n",
    "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
    "graph_builder.add_edge(START, \"chatbot\")\n",
    "\n",
    "memory = InMemorySaver()\n",
    "graph = graph_builder.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe30a205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "I'm learning LangGraph. Could you do some research on it for me?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  tavily_search (ec95e0d3-156a-454e-b77b-b0cc1d7f475a)\n",
      " Call ID: ec95e0d3-156a-454e-b77b-b0cc1d7f475a\n",
      "  Args:\n",
      "    query: LangGraph\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: tavily_search\n",
      "\n",
      "{\"query\": \"LangGraph\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [{\"url\": \"https://www.ibm.com/think/topics/langgraph\", \"title\": \"What is LangGraph? - IBM\", \"content\": \"LangGraph, created by LangChain, is an open source AI agent framework designed to build, deploy and manage complex generative AI agent workflows. At its core, LangGraph uses the power of graph-based architectures to model and manage the intricate relationships between various components of an AI agent workflow. LangGraph illuminates the processes within an AI workflow, allowing full transparency of the agent’s state. By combining these technologies with a set of APIs and tools, LangGraph provides users with a versatile platform for developing AI solutions and workflows including chatbots, state graphs and other agent-based systems. **Nodes**: In LangGraph, nodes represent individual components or agents within an AI workflow. LangGraph uses enhanced decision-making by modeling complex relationships between nodes, which means it uses AI agents to analyze their past actions and feedback.\", \"score\": 0.93666285, \"raw_content\": null}, {\"url\": \"https://towardsdatascience.com/using-langgraph-and-mcp-servers-to-create-my-own-voice-assistant/\", \"title\": \"Using LangGraph and MCP Servers to Create My Own Voice Assistant\", \"content\": \"LangGraph is a framework for building stateful, graph-based workflows with language model agents. Nodes encapsulate any logic related to an\", \"score\": 0.9088853, \"raw_content\": null}], \"response_time\": 0.9, \"request_id\": \"c3782e3d-6d3b-43fc-bac2-fb5f6d1ff130\"}\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "LangGraph, created by LangChain, is an open source AI agent framework designed to build, deploy and manage complex generative AI agent workflows. It uses graph-based architectures to model and manage the intricate relationships between various components of an AI agent workflow, illuminating the processes within an AI workflow and allowing full transparency of the agent’s state. In LangGraph, nodes represent individual components or agents within an AI workflow. LangGraph uses enhanced decision-making by modeling complex relationships between nodes, which means it uses AI agents to analyze their past actions and feedback. It is a framework for building stateful, graph-based workflows with language model agents. Nodes encapsulate any logic related to an agent.\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "events = graph.stream(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": (\n",
    "                    \"I'm learning LangGraph. \"\n",
    "                    \"Could you do some research on it for me?\"\n",
    "                ),\n",
    "            },\n",
    "        ],\n",
    "    },\n",
    "    config,\n",
    "    stream_mode=\"values\",\n",
    ")\n",
    "for event in events:\n",
    "    if \"messages\" in event:\n",
    "        event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b994633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Messages:  4 Next:  ()\n",
      "--------------------------------------------------------------------------------\n",
      "Num Messages:  3 Next:  ('chatbot',)\n",
      "--------------------------------------------------------------------------------\n",
      "Num Messages:  2 Next:  ('tools',)\n",
      "--------------------------------------------------------------------------------\n",
      "Num Messages:  1 Next:  ('chatbot',)\n",
      "--------------------------------------------------------------------------------\n",
      "Num Messages:  0 Next:  ('__start__',)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "to_replay = None\n",
    "for state in graph.get_state_history(config):\n",
    "    print(\"Num Messages: \", len(state.values[\"messages\"]), \"Next: \", state.next)\n",
    "    print(\"-\" * 80)\n",
    "    if len(state.values[\"messages\"]) == 1:\n",
    "    # We are somewhat arbitrarily selecting a specific state based on the number of chat messages in the state.\n",
    "        to_replay = state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b95ff8e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('chatbot',)\n",
      "{'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f08bce5-cd92-6378-8000-d7c8ff014a4e'}}\n"
     ]
    }
   ],
   "source": [
    "if to_replay is not None:\n",
    "\tprint(to_replay.next)\n",
    "\tprint(to_replay.config)\n",
    "else:\n",
    "\tprint(\"No suitable state found (to_replay is None).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "050745e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "I'm learning react. Could you do some research on it for me?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  tavily_search (7a8da2f8-3c20-4f85-bbc1-8ce4b1b53b57)\n",
      " Call ID: 7a8da2f8-3c20-4f85-bbc1-8ce4b1b53b57\n",
      "  Args:\n",
      "    query: react javascript library\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: tavily_search\n",
      "\n",
      "{\"query\": \"react javascript library\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [{\"url\": \"https://en.wikipedia.org/wiki/React_(software)\", \"title\": \"React (software) - Wikipedia\", \"content\": \"React is a free and open-source front-end JavaScript library that aims to make building user interfaces based on components more \\\"seamless\\\".\", \"score\": 0.90771407, \"raw_content\": null}, {\"url\": \"https://kinsta.com/knowledgebase/what-is-react-js/\", \"title\": \"What Is React.js? A Look at the Popular JavaScript Library - Kinsta\", \"content\": \"Jul 11, 2023·React is an open-source, components-based JavaScript library for building fast and dynamic user interfaces. We'll explain how to get\", \"score\": 0.8506799, \"raw_content\": null}], \"response_time\": 1.34, \"request_id\": \"60758510-72db-4b5c-8143-d5be29efe41f\"}\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  tavily_search (d0b903fe-2783-497d-8b03-d21dce2d4e9a)\n",
      " Call ID: d0b903fe-2783-497d-8b03-d21dce2d4e9a\n",
      "  Args:\n",
      "    query: LangGraph framework\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: tavily_search\n",
      "\n",
      "{\"query\": \"LangGraph framework\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [{\"url\": \"https://latenode.com/blog/what-is-langgraph-complete-guide-to-langchains-graph-framework\", \"title\": \"What is LangGraph? Complete Guide to LangChain's Graph ...\", \"content\": \"LangGraph is a framework within LangChain designed for creating dynamic, stateful workflows that go beyond linear processes. Unlike traditional linear workflows, LangGraph uses a graph-based structure with nodes, edges, and shared state. LangGraph's graph-based workflows revolve around three key elements: **nodes**, **edges**, and **shared state**. * **Content Creation Workflows**: For processes involving multiple review cycles, fact-checking, and collaborative editing, LangGraph's state management and human-in-the-loop capabilities shine. > **Key takeaway**: LangGraph is ideal for workflows requiring decisions based on previous outcomes, coordination between multiple agents, or human feedback at various stages. While LangGraph enhances LangChain with robust graph-based workflows, Latenode provides similar stateful, multi-path AI capabilities through an intuitive platform, making it accessible even to those without extensive technical expertise.\", \"score\": 0.9158166, \"raw_content\": null}, {\"url\": \"https://medium.com/@ken_lin/langgraph-a-framework-for-building-stateful-multi-agent-llm-applications-a51d5eb68d03\", \"title\": \"LangGraph: A Framework for Building Stateful Multi-Agent LLM ...\", \"content\": \"# **LangGraph: A Framework for Building Stateful Multi-Agent LLM Applications** As a specialized tool for creating complex LLM applications, LangGraph provides a structured approach to building sophisticated workflows that require cyclical processing patterns. The graph-based structure visually represents how information flows between components, making it easier to design, implement, and reason about complex multi-agent workflows. For straightforward applications that don’t require cyclical workflows or multi-agent coordination, LangGraph may introduce unnecessary complexity. LangGraph represents a significant advancement in the toolkit available for developing sophisticated LLM applications, particularly those involving agent-like behaviors and multi-agent coordination. For organizations and developers looking to build complex agent systems or applications requiring stateful, iterative processing, LangGraph provides a robust foundation that can significantly reduce development time and improve system quality.\", \"score\": 0.90432173, \"raw_content\": null}], \"response_time\": 1.46, \"request_id\": \"4f3c4a9a-5dd9-474b-81c3-007d4c7fa318\"}\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "React is a free and open-source front-end JavaScript library for building user interfaces. LangGraph, a framework within LangChain, is designed for creating dynamic, stateful workflows that go beyond linear processes, using a graph-based structure with nodes, edges, and shared state. It is especially useful for applications requiring cyclical processing patterns and multi-agent coordination.\n"
     ]
    }
   ],
   "source": [
    "# The `checkpoint_id` in the `to_replay.config` corresponds to a state we've persisted to our checkpointer.\n",
    "for event in graph.stream(    \n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": (\n",
    "                    \"I'm learning react. \"\n",
    "                    \"Could you do some research on it for me?\"\n",
    "                ),\n",
    "            },\n",
    "        ],\n",
    "    }, to_replay.config, stream_mode=\"values\"):\n",
    "    if \"messages\" in event:\n",
    "        event[\"messages\"][-1].pretty_print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lang",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
